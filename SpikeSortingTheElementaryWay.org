# -*- org-export-babel-evaluate: nil; ispell-local-dictionary: "american" -*-
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline
#+OPTIONS: author:t c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+TITLE: Spike Sorting The Elementary Way
#+DATE: <2016-01-18 lun.>
#+AUTHOR: Christophe Pouzat
#+EMAIL: christophe.pouzat@parisdescartes.fr
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.3)
#+PROPERTY: header-args:python *Python* :session  :results pp
#+STYLE: <link rel="stylesheet" title="Standard" href="/worg/style/worg.css" type="text/css" />
#+STYLE: <link rel="alternate stylesheet" title="Zenburn" href="/worg/style/worg-zenburn.css" type="text/css" />
#+STYLE: <link rel="alternate stylesheet" title="Classic" href="/worg/style/worg-classic.css" type="text/css" />
#+STYLE: <link rel="stylesheet" href="http://orgmode.org/css/lightbox.css" type="text/css" media="screen" />
#+STYLE: <link rel="SHORTCUT ICON" href="/org-mode-unicorn.ico" type="image/x-icon" />
#+STYLE: <link rel="icon" href="/org-mode-unicorn.ico" type="image/ico" />


#+NAME: emacs-set-up
#+BEGIN_SRC emacs-lisp :results silent :exports none
(setq py-shell-name "~/anaconda3/bin/ipython")

(defun update-tag ()
  (interactive)
  (save-excursion
    (goto-char (point-min))
    (let ((count 1))
      (while (re-search-forward "\\tag{\\([0-9]+\\)}" nil t)
        (replace-match (format "%d" count) nil nil nil 1)
        (setq count (1+ count)))))
  )
#+END_SRC

* Put everything in one =Python= file 				   :noexport:
:PROPERTIES:
:header-args:python: :session *Python*
:END:

#+NAME: make-sure-dir-img/locust-sorting-python-is-here
#+BEGIN_SRC python :results silent :exports none
import os
if not 'figsSorting' in os.listdir("."):
    os.mkdir('figsSorting')

#+END_SRC

#+NAME: make-sure-dir-code-is-here
#+BEGIN_SRC python :results silent :exports none
if not 'code' in os.listdir("."):
    os.mkdir('code')

#+END_SRC

#+name: sorting_with_python
#+BEGIN_SRC python :noweb yes :tangle code/sorting_with_python.py :eval no-export
import numpy as np
import matplotlib.pyplot as plt
import scipy
plt.ion()

<<mad>>

<<plot_data_list>>

<<peak>>

<<cut_sgl_evt>>

<<mk_events>>

<<plot_events>>

<<plot_data_list_and_detection>>

<<mk_noise>>

<<mk_center_dictionary>>

<<mk_aligned_events>>

<<classify_and_align_evt>>

<<predict_data>>
#+END_SRC

* Downloading the data 						     :export:
:PROPERTIES:
:header-args:python: :session *Python*
:END:

The data are available and can be downloaded with (watch out, you must use slightly different commands if you're using =Python 2=): 

#+NAME: download-data
#+BEGIN_SRC python :exports code :results silent
from urllib.request import urlretrieve # Python 3
# from urllib import urlretrieve # Python 2
data_names = ['Locust_' + str(i) + '.dat.gz' for i in range(1,5)]
data_src = ['http://xtof.disque.math.cnrs.fr/data/' + n
            for n in data_names]
[urlretrieve(data_src[i],data_names[i]) for i in range(4)]
#+END_SRC
They were stored as floats coded on 64 bits and compressed with =gnuzip=. So we decompress it:

#+NAME: unzip-data
#+BEGIN_SRC python :results silent
from os import system
[system("gunzip " + fn) for fn in data_names]
#+END_SRC
20 seconds of data sampled at 15 kHz are contained in these files (see [[http://xtof.perso.math.cnrs.fr/pdf/Pouzat+:2002.pdf][PouzatEtAl_2002]] for details). Four
files corresponding to the four electrodes or recording sites of a
/tetrode/ (see Sec. why-tetrode) are used. 

* Importing the required modules and loading the data 		     :export:
:PROPERTIES:
:header-args:python: :session *Python*
:END:

The individual functions developed for this kind of analysis are defined at the end of this document (Sec. [[Individual function definitions]]).
They can also be downloaded as a single file [[https://raw.githubusercontent.com/christophe-pouzat/LASCON2016/master/code/sorting_with_python.py][sorting_with_python.py]] which must then be imported with for instance:

#+NAME: import-swp
#+BEGIN_SRC python :results silent
import sorting_with_python as swp
#+END_SRC 
where it is assumed that the working directory of your =python= session is the directory where the file =sorting_with_python.py= can be found.
We are going to use =numpy= and =pylab= (we will also use =pandas= later on, but to generate only one figure so you can do the analysis without it). We are also going to use the interactive mode of the latter:

#+NAME: setup-np
#+BEGIN_SRC python :results silent
import numpy as np
import matplotlib.pylab as plt
plt.ion()
#+END_SRC

=Python 3= was used to perform this analysis but everything also works with =Python 2=. We load the data with:

#+NAME: load-data
#+BEGIN_SRC python :results silent
# Create a list with the file names
data_files_names = ['Locust_' + str(i) + '.dat' for i in range(1,5)]
# Get the lenght of the data in the files
data_len = np.unique(list(map(len, map(lambda n:
                                       np.fromfile(n,np.double),
                                       data_files_names))))[0]
# Load the data in a list of numpy arrays
data = [np.fromfile(n,np.double) for n in data_files_names]
#+END_SRC

* Preliminary analysis 						     :export:
:PROPERTIES:
:header-args:python: :session *Python*
:END:

We are going to start our analysis by some "sanity checks" to make sure that nothing "weird" happened during the recording.
** Five number summary 
We should start by getting an overall picture of the data like the one provided by the =mquantiles= method of module =scipy.stats.mstats= using it to output a [[http://en.wikipedia.org/wiki/Five-number_summary][five-number summary]]. The five numbers are the =minimum=, the =first quartile=, the =median=, the =third quartile= and the =maximum=:

#+NAME: five-number-summary
#+BEGIN_SRC python :exports both :results pp
from scipy.stats.mstats import mquantiles
np.set_printoptions(precision=3)
[mquantiles(x,prob=[0,0.25,0.5,0.75,1]) for x in data]
#+END_SRC

#+RESULTS: five-number-summary
: [array([ -9.074,  -0.371,  -0.029,   0.326,  10.626]),
:  array([ -8.229,  -0.45 ,  -0.036,   0.396,  11.742]),
:  array([-6.89 , -0.53 , -0.042,  0.469,  9.849]),
:  array([ -7.347,  -0.492,  -0.04 ,   0.431,  10.564])]


In the above result, each row corresponds to a recording channel, the first column contains the minimal value; the second, the first quartile; the third, the median; the fourth, the third quartile; the fifth, the maximal value.
We see that the data range (=maximum - minimum=) is similar (close to 20) on the four recording sites. The inter-quartiles ranges are also similar.

** Were the data normalized?
We can check next if some processing like a division by the /standard deviation/ (SD) has been applied:

#+NAME: data-standard-deviation
#+BEGIN_SRC python :exports both :results pp
[np.std(x) for x in data]
#+END_SRC

#+RESULTS: data-standard-deviation
: [0.99999833333194166,
:  0.99999833333193622,
:  0.99999833333194788,
:  0.99999833333174282]


We see that SD normalization was indeed applied to these data…

** Discretization step amplitude
We can easily obtain the size of the digitization set:

#+NAME: data-discretization-step-amplitude
#+BEGIN_SRC python :exports both :results pp
[np.min(np.diff(np.sort(np.unique(x)))) for x in data]
#+END_SRC

#+RESULTS: data-discretization-step-amplitude
: [0.0067098450784115471,
:  0.0091945001879327748,
:  0.011888432902217971,
:  0.0096140421286605715]

* Plot the data 						     :export:
:PROPERTIES:
:header-args:python: :session *Python*
:END:

Plotting the data for interactive exploration is trivial. The only trick is to add (or subtract) a proper offest (that we get here using the maximal value of each channel from our five-number summary), this is automatically implemented in our =plot_data_list= function:


#+BEGIN_SRC python :results silent
tt = np.arange(0,data_len)/1.5e4
swp.plot_data_list(data,tt,0.1)
#+END_SRC
The first channel is drawn as is, the second is offset downward by the sum of its maximal value and of the absolute value of the minimal value of the first, etc. We then get something like Fig. \ref{fig:WholeRawData}.

#+BEGIN_SRC python :exports results :results file
plt.savefig("figsSorting/locust-sorting-python/WholeRawData.png")
"figsSorting/locust-sorting-python/WholeRawData.png"
#+END_SRC

#+CAPTION: The whole (20 s) Locust antennal lobe data set.
#+ATTR_LATEX: :width 1.0\textwidth
#+NAME: fig:WholeRawData
#+RESULTS:
[[file:figsSorting/locust-sorting-python/WholeRawData.png]]

It is also good to "zoom in" and look at the data with a finer time scale (Fig. \ref{fig:First200ms}) with:

#+BEGIN_SRC python :results silent
plt.xlim([0,0.2])
#+END_SRC

#+BEGIN_SRC python :exports results :results file
plt.savefig("figsSorting/locust-sorting-python/First200ms.png")
plt.close()
"figsSorting/locust-sorting-python/First200ms.png"
#+END_SRC

#+CAPTION: First 200 ms of the Locust data set.
#+NAME: fig:First200ms
#+ATTR_LATEX: :width 1.0\textwidth
#+RESULTS:
[[file:figsSorting/locust-sorting-python/First200ms.png]]


* Individual function definitions 				     :export:
:PROPERTIES:
:header-args:python: :session *Python*
:END:

Short function are presented in 'one piece'. The longer ones are presented with their =docstring= first followed by the body of the function. To get the actual function you should replace the =<<docstring>>= appearing in the function definition by the actual =doctring=. This is just a direct application of the [[http://en.wikipedia.org/wiki/Literate_programming][literate programming]] paradigm. More complicated functions are split into more parts with their own descriptions.
 
** =plot_data_list=
We define a function, =plot_data_list=, making our raw data like displaying command lighter, starting with the =docstring=:

#+name: plot_data_list-doctring
#+BEGIN_SRC python :eval no-export :results silent
"""Plots data when individual recording channels make up elements
of a list.

Parameters
----------
data_list: a list of numpy arrays of dimension 1 that should all
           be of the same length (not checked).
time_axes: an array with as many elements as the components of
           data_list. The time values of the abscissa.
linewidth: the width of the lines drawing the curves.
color: the color of the curves.

Returns
-------
Nothing is returned, the function is used for its side effect: a
plot is generated. 
"""
#+END_SRC
Then the definition of the function per se:

#+name: plot_data_list
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def plot_data_list(data_list,
                   time_axes,
                   linewidth=0.2,
                   color='black'):
    <<plot_data_list-doctring>>
    nb_chan = len(data_list)
    data_min = [np.min(x) for x in data_list]
    data_max = [np.max(x) for x in data_list]
    display_offset = list(np.cumsum(np.array([0] +
                                             [data_max[i]-
                                              data_min[i-1]
                                             for i in
                                             range(1,nb_chan)])))
    for i in range(nb_chan):
        plt.plot(time_axes,data_list[i]-display_offset[i],
                 linewidth=linewidth,color=color)
    plt.yticks([])
    plt.xlabel("Time (s)")

#+END_SRC



** =peak=
We define function =peak= which detects local maxima using an estimate of the derivative of the signal. Only putative maxima that are farther apart than =minimal_dist= sampling points are kept. The function returns a vector of indices. Its =docstring= is:

#+name: peak-docstring
#+BEGIN_SRC python :eval no-export :results silent 
"""Find peaks on one dimensional arrays.

Parameters
----------
x: a one dimensional array on which scipy.signal.fftconvolve can
   be called.
minimal_dist: the minimal distance between two successive peaks.
not_zero: the smallest value above which the absolute value of
the derivative is considered not null.

Returns
-------
An array of (peak) indices is returned.
"""
#+END_SRC
And the function per se:

#+name: peak
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def peak(x, minimal_dist=15, not_zero=1e-3):
    <<peak-docstring>>
    ## Get the first derivative
    dx = scipy.signal.fftconvolve(x,np.array([1,0,-1])/2.,'same') 
    dx[np.abs(dx) < not_zero] = 0
    dx = np.diff(np.sign(dx))
    pos = np.arange(len(dx))[dx < 0]
    return pos[:-1][np.diff(pos) > minimal_dist]

#+END_SRC

** =cut_sgl_evt=

Function =mk_events= (defined next) that we will use directly will call  =cut_sgl_evt=. As its name says cuts a single event (an return a vector with the cuts on the different recording sites glued one after the other). Its =docstring= is:

#+NAME: cut_sgl_evt-docstring
#+BEGIN_SRC python :eval no-export :results silent 
"""Cuts an 'event' at 'evt_pos' on 'data'.
    
Parameters
----------
evt_pos: an integer, the index (location) of the (peak of) the
         event.
data: a matrix whose rows contains the recording channels.
before: an integer, how many points should be within the cut
        before the reference index / time given by evt_pos.
after: an integer, how many points should be within the cut
       after the reference index / time given by evt_pos.
    
Returns
-------
A vector with the cuts on the different recording sites glued
one after the other. 
"""
#+END_SRC
And the function per se:

#+name: cut_sgl_evt
#+BEGIN_SRC python :eval no-export :results silent :no-web no-export 
def cut_sgl_evt(evt_pos,data,before=14, after=30):
    <<cut_sgl_evt-docstring>>
    ns = data.shape[0] ## Number of recording sites
    dl = data.shape[1] ## Number of sampling points
    cl = before+after+1 ## The length of the cut
    cs = cl*ns ## The 'size' of a cut
    cut = np.zeros((ns,cl))
    idx = np.arange(-before,after+1)
    keep = idx + evt_pos
    within = np.bitwise_and(0 <= keep, keep < dl)
    kw = keep[within]
    cut[:,within] = data[:,kw].copy()
    return cut.reshape(cs) 
  
#+END_SRC

** =mk_events=
Function =mk_events= takes a vector of indices as its first argument and returns a matrix with has many rows as events. Its =docstring is=

#+NAME: mk_events-docstring
#+BEGIN_SRC python :eval no-export :results silent 
"""Make events matrix out of data and events positions.
    
Parameters
----------
positions: a vector containing the indices of the events.
data: a matrix whose rows contains the recording channels.
before: an integer, how many points should be within the cut
        before the reference index / time given by evt_pos.
after: an integer, how many points should be within the cut
       after the reference index / time given by evt_pos.
    
Returns
-------
A matrix with as many rows as events and whose rows are the cuts
on the different recording sites glued one after the other. 
"""
#+END_SRC
And the function per se:

#+name: mk_events
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def mk_events(positions, data, before=14, after=30):
    <<mk_events-docstring>>
    res = np.zeros((len(positions),(before+after+1)*data.shape[0]))
    for i,p in enumerate(positions):
        res[i,:] = cut_sgl_evt(p,data,before,after)
    return res 

#+END_SRC

** =plot_events=
In order to facilitate events display, we define an event specific plotting function starting with its =docstring=:

#+name: plot_events-docstring
#+BEGIN_SRC python :eval no-export :results silent 
"""Plot events.
    
Parameters
----------
evts_matrix: a matrix of events. Rows are events. Cuts from
             different recording sites are glued one after the
             other on each row.
n_plot: an integer, the number of events to plot (if 'None',
        default, all are shown).
n_channels: an integer, the number of recording channels.
events_color: the color used to display events. 
events_lw: the line width used to display events. 
show_median: should the median event be displayed?
median_color: color used to display the median event.
median_lw: line width used to display the median event.
show_mad: should the MAD be displayed?
mad_color: color used to display the MAD.
mad_lw: line width used to display the MAD.

Returns
-------
Noting, the function is used for its side effect.
"""
#+END_SRC
And the function per se:

#+name: plot_events
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def plot_events(evts_matrix, 
                n_plot=None,
                n_channels=4,
                events_color='black', 
                events_lw=0.1,
                show_median=True,
                median_color='red',
                median_lw=0.5,
                show_mad=True,
                mad_color='blue',
                mad_lw=0.5):
    <<plot_events-docstring>>
    if n_plot is None:
        n_plot = evts_matrix.shape[0]

    cut_length = evts_matrix.shape[1] // n_channels 
    
    for i in range(n_plot):
        plt.plot(evts_matrix[i,:], color=events_color, lw=events_lw)
    if show_median:
        MEDIAN = np.apply_along_axis(np.median,0,evts_matrix)
        plt.plot(MEDIAN, color=median_color, lw=median_lw)

    if show_mad:
        MAD = np.apply_along_axis(mad,0,evts_matrix)
        plt.plot(MAD, color=mad_color, lw=mad_lw)
    
    left_boundary = np.arange(cut_length,
                              evts_matrix.shape[1],
                              cut_length*2)
    for l in left_boundary:
        plt.axvspan(l,l+cut_length-1,
                    facecolor='grey',alpha=0.5,edgecolor='none')
    plt.xticks([])
    return

#+END_SRC

** =plot_data_list_and_detection=
We define a function, =plot_data_list_and_detection=, making our data and detection displaying command lighter. Its =docstring=:

#+name: plot_data_list_and_detection-docstring
#+BEGIN_SRC python :eval no-export :results silent
"""Plots data together with detected events.
    
Parameters
----------
data_list: a list of numpy arrays of dimension 1 that should all
           be of the same length (not checked).
time_axes: an array with as many elements as the components of
           data_list. The time values of the abscissa.
evts_pos: a vector containing the indices of the detected
          events.
linewidth: the width of the lines drawing the curves.
color: the color of the curves.

Returns
-------
Nothing is returned, the function is used for its side effect: a
plot is generated. 
"""
#+END_SRC
And the function:

#+name: plot_data_list_and_detection
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def plot_data_list_and_detection(data_list,
                                 time_axes,
                                 evts_pos,
                                 linewidth=0.2,
                                 color='black'):                             
    <<plot_data_list_and_detection-docstring>>
    nb_chan = len(data_list)
    data_min = [np.min(x) for x in data_list]
    data_max = [np.max(x) for x in data_list]
    display_offset = list(np.cumsum(np.array([0] +
                                             [data_max[i]-
                                              data_min[i-1] for i in
                                             range(1,nb_chan)])))
    for i in range(nb_chan):
        plt.plot(time_axes,data_list[i]-display_offset[i],
                 linewidth=linewidth,color=color)
        plt.plot(time_axes[evts_pos],
                 data_list[i][evts_pos]-display_offset[i],'ro')
    plt.yticks([])
    plt.xlabel("Time (s)")

#+END_SRC

** =mk_noise=
Getting an estimate of the noise statistical properties is an essential ingredient to build respectable goodness of fit tests. In our approach "noise events" are essentially anything that is not an "event". I wrote essentially and not exactly since there is a little twist here which is the minimal distance we are willing to accept between the reference time of a noise event and the reference time of the last preceding and of the first following "event". We could think that keeping a cut length on each side would be enough. That would indeed be the case if /all/ events were starting from and returning to zero within a cut. But this is not the case with the cuts parameters we chose previously (that will become clear soon). You might wonder why we chose so short a cut length then. Simply to avoid having to deal with too many superposed events which are the really bothering events for anyone wanting to do proper sorting. 
To obtain our noise events we are going to use function =mk_noise= which takes the /same/ arguments as function =mk_events= plus two numbers: 
+ =safety_factor= a number by which the cut length is multiplied and which sets the minimal distance between the reference times discussed in the previous paragraph.
+ =size= the maximal number of noise events one wants to cut (the actual number obtained might be smaller depending on the data length, the cut length, the safety factor and the number of events).

We define now function =mk_noise= starting with its =docstring=:

#+name: mk_noise-docstring
#+BEGIN_SRC python :eval no-export :results silent
"""Constructs a noise sample.

Parameters
----------
positions: a vector containing the indices of the events.
data: a matrix whose rows contains the recording channels.
before: an integer, how many points should be within the cut
        before the reference index / time given by evt_pos.
after: an integer, how many points should be within the cut
       after the reference index / time given by evt_pos.
safety_factor: a number by which the cut length is multiplied
               and which sets the minimal distance between the 
               reference times discussed in the previous
               paragraph.
size: the maximal number of noise events one wants to cut (the
      actual number obtained might be smaller depending on the
      data length, the cut length, the safety factor and the
      number of events).
    
Returns
-------
A matrix with as many rows as noise events and whose rows are
the cuts on the different recording sites glued one after the
other. 
"""
#+END_SRC
And the function:

#+name: mk_noise
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def mk_noise(positions, data, before=14, after=30, safety_factor=2, size=2000):
    <<mk_noise-docstring>>
    sl = before+after+1 ## cut length
    ns = data.shape[0] ## number of recording sites
    i1 = np.diff(positions) ## inter-event intervals
    minimal_length = round(sl*safety_factor)
    ## Get next the number of noise sweeps that can be
    ## cut between each detected event with a safety factor
    nb_i = (i1-minimal_length)//sl
    ## Get the number of noise sweeps that are going to be cut
    nb_possible = min(size,sum(nb_i[nb_i>0]))
    res = np.zeros((nb_possible,sl*data.shape[0]))
    ## Create next a list containing the indices of the inter event
    ## intervals that are long enough
    idx_l = [i for i in range(len(i1)) if nb_i[i] > 0]
    ## Make next an index running over the inter event intervals
    ## from which at least one noise cut can be made
    interval_idx = 0
    ## noise_positions = np.zeros(nb_possible,dtype=numpy.int)
    n_idx = 0
    while n_idx < nb_possible:
        within_idx = 0 ## an index of the noise cut with a long enough
                       ## interval
        i_pos = positions[idx_l[interval_idx]] + minimal_length
        ## Variable defined next contains the number of noise cuts
        ## that can be made from the "currently" considered long-enough
        ## inter event interval
        n_at_interval_idx = nb_i[idx_l[interval_idx]]
        while within_idx < n_at_interval_idx and n_idx < nb_possible:
            res[n_idx,:]= cut_sgl_evt(int(i_pos),data,before,after)
            ## noise_positions[n_idx] = i_pos
            n_idx += 1
            i_pos += sl
            within_idx += 1
        interval_idx += 1
    ## return (res,noise_positions)
    return res

#+END_SRC

** =mad=
We define the =mad= function in one piece since it is very short:

#+name: mad
#+BEGIN_SRC python :eval no-export :results silent
def mad(x):
    """Returns the Median Absolute Deviation of its argument.
    """
    return np.median(np.absolute(x - np.median(x)))*1.4826

#+END_SRC

** =mk_aligned_events=
*** The jitter: A worked out example
Function =mk_aligned_events= is somehow the "heavy part" of this document. Its job is to align events on their templates while taking care of two jitter sources: the sampling and the noise one. Rather than getting into a theoretical discussion, we illustrate the problem with one of the events detected on our data set. Cluster 1 is the cluster exhibiting the largest [[http://en.wikipedia.org/wiki/Jitter][sampling jitter]] effects, since it has the largest time derivative, in absolute value, of its median event . This is clearly seen when we superpose the 50th event from this cluster with the median event (remember that we start numbering at 0). So we get first our estimate for center or template of cluster 1:

#+NAME: c1_median
#+BEGIN_SRC python :session *Python* :results silent
c1_median = apply(np.median,0,evtsE[goodEvts,:][np.array(c10b)==1,:])
#+END_SRC
And we do the plot (Fig. \ref{fig:JitterIllustrationCluster1Event50}):

#+BEGIN_SRC python :session *Python* :results silent
plt.plot(c1_median,color='red')
plt.plot(evtsE[goodEvts,:][np.array(c10b)==1,:][50,:],color='black')
#+END_SRC

#+BEGIN_SRC python :session *Python*  :exports results :results file
plt.savefig('img/locust-sorting-python/JitterIllustrationCluster1Event50.png')
plt.close()
'img/locust-sorting-python/JitterIllustrationCluster1Event50.png'
#+END_SRC

#+CAPTION: The median event of cluster 1 (red) together with event 50 of the same cluster (black).
#+NAME: fig:JitterIllustrationCluster1Event50
#+ATTR_LaTeX: :width 0.8\textwidth
#+RESULTS:
[[file:img/locust-sorting-python/JitterIllustrationCluster1Event50.png]]

A Taylor expansion shows that if we write /g(t)/ the observed 50th event, δ the sampling jitter and /f(t)/ the actual waveform of the event then:
\begin{equation}
g(t) = f(t+δ) + ε(t) \approx f(t) + δ \, f'(t) + δ^2/2 \, f''(t) + ε(t) \, ;
\end{equation}
where ε is a Gaussian process and where $f'$ and $f''$ stand for the first and second time derivatives of $f$. Therefore, if we can get estimates of $f'$ and $f''$ we should be able to estimate δ by linear regression (if we neglect the $δ^2$ term as well as the potentially non null correlation in ε) or by non linear regression (if we keep the latter). We start by getting the derivatives estimates:

#+NAME: c1D_median-and-c1DD_median
#+BEGIN_SRC python :session *Python* :results silent 
dataD = apply(lambda x: fftconvolve(x,np.array([1,0,-1])/2.,'same'),
              1, data)
evtsED = swp.mk_events(sp0E,dataD,14,30)
dataDD = apply(lambda x: fftconvolve(x,np.array([1,0,-1])/2.,'same'),
               1, dataD)
evtsEDD = swp.mk_events(sp0E,dataDD,14,30)
c1D_median = apply(np.median,0,
                   evtsED[goodEvts,:][np.array(c10b)==1,:])
c1DD_median = apply(np.median,0,
                    evtsEDD[goodEvts,:][np.array(c10b)==1,:])
#+END_SRC
We then get something like Fig. \ref{fig:JitterIllustrationCluster1Event50b}:

#+BEGIN_SRC python :session *Python*  :results silent
plt.plot(evtsE[goodEvts,:][np.array(c10b)==1,:][50,:]-\
         c1_median,color='red',lw=2)
plt.plot(1.5*c1D_median,color='blue',lw=2)
plt.plot(1.5*c1D_median+1.5**2/2*c1DD_median,color='black',lw=2)
#+END_SRC

#+BEGIN_SRC python :session *Python*  :exports results :results file
plt.savefig('img/locust-sorting-python/JitterIllustrationCluster1Event50b.png')
plt.close()
'img/locust-sorting-python/JitterIllustrationCluster1Event50b.png'
#+END_SRC

#+CAPTION: The median event of cluster 1 subtracted from event 50 of the same cluster (red); 1.5 times the first derivative of the median event (blue)—corresponding to δ=1.5—; 1.5 times the first derivative + 1.5^2/2 times the second (black)—corresponding again to δ=1.5—.
#+NAME: fig:JitterIllustrationCluster1Event50b
#+ATTR_LaTeX: :width 0.8\textwidth
#+RESULTS:
[[file:img/locust-sorting-python/JitterIllustrationCluster1Event50b.png]]

If we neglect the $δ^2$ term we quickly arrive at:
\begin{equation}
\hat{δ} = \frac{\mathbf{f'} \cdot (\mathbf{g} -\mathbf{f})}{\| \mathbf{f'} \|^2} \, ;
\end{equation} 
where the 'vectorial' notation like $\mathbf{a} \cdot \mathbf{b}$ stands here for: 
\[
\sum_{i=0}^{179} a_i b_i \, .
\]

For the 50th event of the cluster we get:

#+NAME: delta_hat
#+BEGIN_SRC python :session *Python*  :results pp :exports both
delta_hat = np.dot(c1D_median,
                   evtsE[goodEvts,:][np.array(c10b)==1,:][50,:]-\
                   c1_median)/np.dot(c1D_median,c1D_median)
delta_hat
#+END_SRC

#+RESULTS: delta_hat
: 1.4917182304326999

We can use this estimated value of =delta_hat= as an initial guess for a procedure refining the estimate using also the $δ^2$ term. The obvious quantity we should try to minimize is the residual sum of square, =RSS= defined by:
\[
\mathrm{RSS}(δ) = \| \mathbf{g} - \mathbf{f} - δ \, \mathbf{f'} - δ^2/2 \, \mathbf{f''} \|^2 \; .
\]
We can define a function returning the =RSS= for a given value of δ as well as an event =evt= a cluster center (median event of the cluster) =center= and its first two derivatives, =centerD= and =centerDD=:

#+NAME: rss_fct
#+BEGIN_SRC python :session *Python* :results silent
def rss_fct(delta,evt,center,centerD,centerDD):
    return np.sum((evt - center - delta*centerD - delta**2/2*centerDD)**2)

#+END_SRC  
To create quickly a graph of the =RSS= as a function of δ for the specific case we are dealing with now (51st element of cluster 1) we create a vectorized or /universal/ function version of the =rss_for_alignment= we just defined:

#+NAME: urss_fct
#+BEGIN_SRC python :session *Python* :results silent 
urss_fct = np.frompyfunc(lambda x:
                         rss_fct(x,
                                 evtsE[goodEvts,:]\
                                 [np.array(c10b)==1,:][50,:],
                                 c1_median,c1D_median,c1DD_median),1,1)

#+END_SRC  
We then get the Fig. \ref{fig:JitterIllustrationCluster1Event50c} with:

#+BEGIN_SRC python :session *Python* :results silent
plt.subplot(1,2,1)
dd = np.arange(-5,5,0.05)
plt.plot(dd,urss_fct(dd),color='black',lw=2)
plt.subplot(1,2,2)
dd_fine = np.linspace(delta_hat-0.5,delta_hat+0.5,501)
plt.plot(dd_fine,urss_fct(dd_fine),color='black',lw=2)
plt.axvline(x=delta_hat,color='red')
#+END_SRC

#+BEGIN_SRC python :session *Python*  :exports results :results file
plt.savefig('img/locust-sorting-python/JitterIllustrationCluster1Event50c.png')
plt.close()
'img/locust-sorting-python/JitterIllustrationCluster1Event50c.png'
#+END_SRC

#+CAPTION: The =RSS= as a function of δ for event 50 of cluster 1. Left, $δ \in [-5,5]$; right, $δ \in [\hat{δ}-0.5,\hat{δ}+0.5]$ and the red vertical line shows $\hat{δ}$. 
#+NAME: fig:JitterIllustrationCluster1Event50c
#+ATTR_LaTeX: :width 0.8\textwidth
#+RESULTS:
[[file:img/locust-sorting-python/JitterIllustrationCluster1Event50c.png]]

The left panel of the above figure shows that our initial guess for $\hat{δ}$ is not bad but still approximately 0.2 units away from the actual minimum. The classical way to refine our δ estimate—in 'nice situations' where the function we are trying to minimize is locally convex—is to use the [[http://en.wikipedia.org/wiki/Newton%27s_method][Newton-Raphson algorithm]] which consists in approximating locally the 'target function' (here our =RSS= function) by a parabola having locally the same first and second derivatives, before jumping to the minimum of this approximating parabola. If we develop our previous expression of $\mathrm{RSS}(δ)$ we get:
\[
\mathrm{RSS}(δ) = \| \mathbf{h} \|^2 - 2\, δ \, \mathbf{h} \cdot \mathbf{f'} + δ^2 \, \left( \|\mathbf{f'}\|^2 -  \mathbf{h} \cdot \mathbf{f''}\right) + δ^3 \, \mathbf{f'} \cdot \mathbf{f''} + \frac{δ^4}{4} \|\mathbf{f''}\|^2 \, ;
\]
where $\mathbf{h}$ stands for $\mathbf{g} - \mathbf{f}$. By differentiation with respect to δ we get:
\[
\mathrm{RSS}'(δ) = - 2\, \mathbf{h} \cdot \mathbf{f'} + 2 \, δ \, \left( \|\mathbf{f'}\|^2 -  \mathbf{h} \cdot \mathbf{f''}\right) + 3 \, δ^2 \, \mathbf{f'} \cdot \mathbf{f''} + δ^3 \|\mathbf{f''}\|^2 \, .
\]
And a second differentiation leads to:
\[
\mathrm{RSS}''(δ) = 2 \, \left( \|\mathbf{f'}\|^2 -  \mathbf{h} \cdot \mathbf{f''}\right) + 6 \, δ \, \mathbf{f'} \cdot \mathbf{f''} + 3 \, δ^2 \|\mathbf{f''}\|^2 \, .
\]
The equation of the approximating parabola at $δ^{(k)}$ is then:
\[
\mathrm{RSS}(δ^{(k)} + η) \approx \mathrm{RSS}(δ^{(k)}) + η \, \mathrm{RSS}'(δ^{(k)}) + \frac{η^2}{2} \, \mathrm{RSS}''(δ^{(k)})\; ,
\]
and its minimum—if $\mathrm{RSS}''(δ)$ > 0—is located at:
\[
δ^{(k+1)} = δ^{(k)} - \frac{\mathrm{RSS}'(δ^{(k)})}{\mathrm{RSS}''(δ^{(k)})} \; .
\]
Defining functions returning the required derivatives:

#+NAME: rssD_fct-and-rssDD_fct
#+BEGIN_SRC python :session *Python*  :results silent
def rssD_fct(delta,evt,center,centerD,centerDD):
    h = evt - center
    return -2*np.dot(h,centerD) + \
      2*delta*(np.dot(centerD,centerD) - np.dot(h,centerDD)) + \
      3*delta**2*np.dot(centerD,centerDD) + \
      delta**3*np.dot(centerDD,centerDD)

def rssDD_fct(delta,evt,center,centerD,centerDD):
    h = evt - center
    return 2*(np.dot(centerD,centerD) - np.dot(h,centerDD)) + \
      6*delta*np.dot(centerD,centerDD) + \
      3*delta**2*np.dot(centerDD,centerDD)

#+END_SRC
we can get a graphical representation (Fig. \ref{fig:JitterIllustrationCluster1Event50d}) of a single step of the Newton-Raphson algorithm:

#+NAME: delta_1 
#+BEGIN_SRC python :session *Python* :results silent
rss_at_delta0 = rss_fct(delta_hat,
                        evtsE[goodEvts,:][np.array(c10b)==1,:][50,:],
                        c1_median,c1D_median,c1DD_median)
rssD_at_delta0 = rssD_fct(delta_hat,
                          evtsE[goodEvts,:][np.array(c10b)==1,:][50,:],
                          c1_median,c1D_median,c1DD_median)
rssDD_at_delta0 = rssDD_fct(delta_hat,
                            evtsE[goodEvts,:][np.array(c10b)==1,:]\
                            [50,:],c1_median,c1D_median,c1DD_median)
delta_1 = delta_hat - rssD_at_delta0/rssDD_at_delta0
#+END_SRC

#+BEGIN_SRC python :session *Python* :results silent
plt.plot(dd_fine,urss_fct(dd_fine),color='black',lw=2)
plt.axvline(x=delta_hat,color='red')
plt.plot(dd_fine,
         rss_at_delta0 + (dd_fine-delta_hat)*rssD_at_delta0 + \
         (dd_fine-delta_hat)**2/2*rssDD_at_delta0,color='blue',lw=2)
plt.axvline(x=delta_1,color='grey')
#+END_SRC
#+BEGIN_SRC python :session *Python*  :exports results :results file
plt.savefig('img/locust-sorting-python/JitterIllustrationCluster1Event50d.png')
plt.close()
'img/locust-sorting-python/JitterIllustrationCluster1Event50d.png'
#+END_SRC

#+CAPTION: The =RSS= as a function of δ for event 50 of cluster 1  (black), the red vertical line shows $\hat{δ}$. In blue, the approximating parabola at $\hat{δ}$. The grey vertical line shows the minimum of the approximating parabola.
#+NAME: fig:JitterIllustrationCluster1Event50d
#+ATTR_LaTeX: :width 0.8\textwidth
#+RESULTS:
[[file:img/locust-sorting-python/JitterIllustrationCluster1Event50d.png]]

Subtracting the second order in δ approximation of f(t+δ) from the observed 50th event of cluster 1 we get Fig. \ref{fig:JitterIllustrationCluster1Event50e}:

#+BEGIN_SRC python :session *Python* :results silent
plt.plot(evtsE[goodEvts,:][np.array(c10b)==1,:][50,:]-\
         c1_median-delta_1*c1D_median-delta_1**2/2*c1DD_median,
         color='red',lw=2)
plt.plot(evtsE[goodEvts,:][np.array(c10b)==1,:][50,:],
         color='black',lw=2)
plt.plot(c1_median+delta_1*c1D_median+delta_1**2/2*c1DD_median,
         color='blue',lw=1)
#+END_SRC 
#+BEGIN_SRC python :session *Python*  :exports results :results file
plt.savefig('img/locust-sorting-python/JitterIllustrationCluster1Event50e.png')
plt.close()
'img/locust-sorting-python/JitterIllustrationCluster1Event50e.png'
#+END_SRC

#+CAPTION: Event 50 of cluster 1 (black), second order approximation of f(t+δ) (blue) and residual (red) for δ—obtained by a succession of a linear regression (order 1) and a single Newton-Raphson step—equal to: src_python[:session *Python*  :results pp]{delta_1} =1.3748048144324905=.
#+NAME: fig:JitterIllustrationCluster1Event50e
#+ATTR_LaTeX: :width 0.8\textwidth
#+RESULTS:
[[file:img/locust-sorting-python/JitterIllustrationCluster1Event50e.png]]

*** Function definition

We start with the chunk importing the required functions from the different modules (=<<mk_aligned_events-import-functions>>=):

#+NAME: mk_aligned_events-import-functions
#+BEGIN_SRC python :eval no-export
from scipy.signal import fftconvolve
from numpy import apply_along_axis as apply
from scipy.spatial.distance import squareform
#+END_SRC
We then get the first and second derivatives of the data:

#+NAME: mk_aligned_events-dataD-and-dataDD
#+BEGIN_SRC python :eval no-export
dataD = apply(lambda x: fftconvolve(x,np.array([1,0,-1])/2., 'same'),
              1, data)
dataDD = apply(lambda x: fftconvolve(x,np.array([1,0,-1])/2.,'same'),
               1, dataD)
    
#+END_SRC
Events are cut from the different data 'versions', derivatives of order 0, 1 and 2 (=<<mk_aligned_events-get-events>>=):

#+NAME: mk_aligned_events-get-events
#+BEGIN_SRC python :eval no-export
evts = mk_events(positions, data, before, after)
evtsD = mk_events(positions, dataD, before, after)
evtsDD = mk_events(positions, dataDD, before, after)    
#+END_SRC
A center or template is obtained by taking the pointwise median of the events we just got on the three versions of the data (=<<mk_aligned_events-get-centers>>=):

#+NAME: mk_aligned_events-get-centers
#+BEGIN_SRC python :eval no-export
center = apply(np.median,0,evts)
centerD = apply(np.median,0,evtsD)
centerD_norm2 = np.dot(centerD,centerD)
centerDD = apply(np.median,0,evtsDD)
centerDD_norm2 = np.dot(centerDD,centerDD)
centerD_dot_centerDD = np.dot(centerD,centerDD)
#+END_SRC
Given an event, make a first order jitter estimation and compute the norm of the initial residual, =h_order0_norm2=, and of its first order jitter corrected version, =h_order1_norm2= (=<<mk_aligned_events-do-job-on-single-event-order1>>=):

#+NAME: mk_aligned_events-do-job-on-single-event-order1
#+BEGIN_SRC python :eval no-export
h = evt - center
h_order0_norm2 = sum(h**2)
h_dot_centerD = np.dot(h,centerD)
jitter0 = h_dot_centerD/centerD_norm2
h_order1_norm2 = sum((h-jitter0*centerD)**2)
#+END_SRC
If the residual's norm decrease upon first order jitter correction, try a second order one. At the end compare the norm of the second order jitter corrected residual (=h_order2_norm2=) with the one of the first order (=h_order1_norm2=). If the former is larger or equal than the latter, set the estimated jitter to its first order value (=<<mk_aligned_events-do-job-on-single-event-order2>>=): 

#+NAME: mk_aligned_events-do-job-on-single-event-order2
#+BEGIN_SRC python :eval no-export
h_dot_centerDD = np.dot(h,centerDD)
first = -2*h_dot_centerD + \
  2*jitter0*(centerD_norm2 - h_dot_centerDD) + \
  3*jitter0**2*centerD_dot_centerDD + \
  jitter0**3*centerDD_norm2
second = 2*(centerD_norm2 - h_dot_centerDD) + \
  6*jitter0*centerD_dot_centerDD + \
  3*jitter0**2*centerDD_norm2
jitter1 = jitter0 - first/second
h_order2_norm2 = sum((h-jitter1*centerD- \
                      jitter1**2/2*centerDD)**2)
if h_order1_norm2 <= h_order2_norm2:
    jitter1 = jitter0
#+END_SRC
And now the function's =docstring= (=<<mk_aligned_events-docstring>>=):

#+NAME: mk_aligned_events-docstring
#+BEGIN_SRC python :eval no-export
"""Align events on the central event using first or second order
Taylor expansion.

Parameters
----------
positions: a vector of indices with the positions of the
           detected events. 
data: a matrix whose rows contains the recording channels.
before: an integer, how many points should be within the cut
        before the reference index / time given by positions.
after: an integer, how many points should be within the cut
       after the reference index / time given by positions.
   
Returns
-------
A tuple whose elements are:
  A matrix with as many rows as events and whose rows are the
  cuts on the different recording sites glued one after the
  other. These events have been jitter corrected using the
  second order Taylor expansion.
  A vector of events positions where "actual" positions have
  been rounded to the nearest index.
  A vector of jitter values.
  
Details
------- 
(1) The data first and second derivatives are estimated first.
(2) Events are cut next on each of the three versions of the data.
(3) The global median event for each of the three versions are
obtained.
(4) Each event is then aligned on the median using a first order
Taylor expansion.
(5) If this alignment decreases the squared norm of the event
(6) an improvement is looked for using a second order expansion.
If this second order expansion still decreases the squared norm
and if the estimated jitter is larger than 1, the whole procedure
is repeated after cutting a new the event based on a better peak
position (7). 
"""
#+END_SRC
To end up with the function itself:

#+name: mk_aligned_events
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def mk_aligned_events(positions, data, before=14, after=30):
    <<mk_aligned_events-docstring>>
    <<mk_aligned_events-import-functions>>
    n_evts = len(positions)
    new_positions = positions.copy()
    jitters = np.zeros(n_evts)
    # Details (1)
    <<mk_aligned_events-dataD-and-dataDD>>
    # Details (2)
    <<mk_aligned_events-get-events>>
    # Details (3)
    <<mk_aligned_events-get-centers>>
    # Details (4)
    for evt_idx in range(n_evts):
        # Details (5)
        evt = evts[evt_idx,:]
        evt_pos = positions[evt_idx]
        <<mk_aligned_events-do-job-on-single-event-order1>>
        if h_order0_norm2 > h_order1_norm2:
            # Details (6)
            <<mk_aligned_events-do-job-on-single-event-order2>>
        else:
            jitter1 = 0
        if abs(round(jitter1)) > 0:
            # Details (7)
            evt_pos -= int(round(jitter1))
            evt = cut_sgl_evt(evt_pos,data=data,
                              before=before, after=after)
            <<mk_aligned_events-do-job-on-single-event-order1>>		      
            if h_order0_norm2 > h_order1_norm2:
                <<mk_aligned_events-do-job-on-single-event-order2>>
            else:
                jitter1 = 0
        if sum(evt**2) > sum((h-jitter1*centerD-
                              jitter1**2/2*centerDD)**2):
            evts[evt_idx,:] = evt-jitter1*centerD- \
                jitter1**2/2*centerDD
        new_positions[evt_idx] = evt_pos 
        jitters[evt_idx] = jitter1
    return (evts, new_positions,jitters)

#+END_SRC


** =mk_center_dictionary= :export:
We define function =mk_center_dictionary= starting with its =docstring=:

#+NAME: mk_center_dictionary-docstring
#+BEGIN_SRC python :eval no-export
""" Computes clusters 'centers' or templates and associated data.

Clusters' centers should be built such that they can be used for 
subtraction, this implies that we should make them long enough, on
both side of the peak, to see them go back to baseline. Formal
parameters before and after bellow should therefore be set to
larger values than the ones used for clustering. 

Parameters
----------
positions : a vector of spike times, that should all come from the
            same cluster and correspond to reasonably 'clean'
            events.
data : a data matrix.
before : the number of sampling point to keep before the peak.
after : the number of sampling point to keep after the peak.

Returns
-------
A dictionary with the following components:
  center: the estimate of the center (obtained from the median).
  centerD: the estimate of the center's derivative (obtained from
           the median of events cut on the derivative of data).
  centerDD: the estimate of the center's second derivative
            (obtained from the median of events cut on the second
            derivative of data).
  centerD_norm2: the squared norm of the center's derivative.
  centerDD_norm2: the squared norm of the center's second
                  derivative.
  centerD_dot_centerDD: the scalar product of the center's first
                        and second derivatives.
  center_idx: an array of indices generated by
              np.arange(-before,after+1).
 """
#+END_SRC
The function starts by evaluating the first two derivatives of the data (=<<get-derivatives>>=):

#+NAME: mk_center_dictionary-get-derivatives
#+BEGIN_SRC python :eval never
from scipy.signal import fftconvolve
from numpy import apply_along_axis as apply
dataD = apply(lambda x:
              fftconvolve(x,np.array([1,0,-1])/2.,'same'),
              1, data)
dataDD = apply(lambda x:
               fftconvolve(x,np.array([1,0,-1])/2.,'same'),
               1, dataD)
    
#+END_SRC
The function is defined next:

#+name: mk_center_dictionary
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def mk_center_dictionary(positions, data, before=49, after=80):
    <<mk_center_dictionary-docstring>>
    <<mk_center_dictionary-get-derivatives>>
    evts = mk_events(positions, data, before, after)
    evtsD = mk_events(positions, dataD, before, after)
    evtsDD = mk_events(positions, dataDD, before, after)
    evts_median = apply(np.median,0,evts)
    evtsD_median = apply(np.median,0,evtsD)
    evtsDD_median = apply(np.median,0,evtsDD)
    return {"center" : evts_median, 
            "centerD" : evtsD_median, 
            "centerDD" : evtsDD_median, 
            "centerD_norm2" : np.dot(evtsD_median,evtsD_median),
            "centerDD_norm2" : np.dot(evtsDD_median,evtsDD_median),
            "centerD_dot_centerDD" : np.dot(evtsD_median,
                                            evtsDD_median), 
            "center_idx" : np.arange(-before,after+1)}

#+END_SRC

** =classify_and_align_evt=
We now define with the following =docstring= (=<<classify_and_align_evt-docstring>>=):

#+NAME: classify_and_align_evt-docstring
#+BEGIN_SRC python :eval no-export
"""Compares a single event to a dictionary of centers and returns
the name of the closest center if it is close enough or '?', the
corrected peak position and the remaining jitter.

Parameters
----------
evt_pos : a sampling point at which an event was detected.
data : a data matrix.
centers : a centers' dictionary returned by mk_center_dictionary.
before : the number of sampling point to consider before the peak.
after : the number of sampling point to consider after the peak.

Returns
-------
A list with the following components:
  The name of the closest center if it was close enough or '?'.
  The nearest sampling point to the events peak.
  The jitter: difference between the estimated actual peak
  position and the nearest sampling point.
"""
#+END_SRC
The first chunk of the function takes a dictionary of centers, =centers=, generated by =mk_center_dictionary=, defines two variables, =cluster_names= and =n_sites=, and builds a matrix of centers, =centersM=:

#+NAME: classify_and_align_evt-centersM
#+BEGIN_SRC python :eval no-export
cluster_names = np.sort(list(centers))
n_sites = data.shape[0]
centersM = np.array([centers[c_name]["center"]\
                     [np.tile((-before <= centers[c_name]\
                               ["center_idx"]).\
                               __and__(centers[c_name]["center_idx"] \
                                       <= after), n_sites)]
                                       for c_name in cluster_names])
#+END_SRC
Extract the event, =evt=, to classify and subtract each center from it, =delta=, to find the closest one, =cluster_idx=, using the Euclidean squared norm (=<<cluster_idx>>=):

#+NAME: classify_and_align_evt-cluster_idx
#+BEGIN_SRC python :eval no-export
evt = cut_sgl_evt(evt_pos,data=data,before=before, after=after)
delta = -(centersM - evt)
cluster_idx = np.argmin(np.sum(delta**2,axis=1))    
#+END_SRC
Get the name of the selected cluster, =good_cluster_name=, and its 'time indices', =good_cluster_idx=. Then, extract the first two derivatives of the center, =centerD= and =centerDD=, their squared norms, =centerD_norm2= and =centerDD_norm2=, and their dot product, =centerD_dot_centerDD= (=<<get-centers>>=):

#+NAME: classify_and_align_evt-get-centers
#+BEGIN_SRC python :eval no-export
good_cluster_name = cluster_names[cluster_idx]
good_cluster_idx = np.tile((-before <= centers[good_cluster_name]\
                            ["center_idx"]).\
                            __and__(centers[good_cluster_name]\
                                    ["center_idx"] <= after),
                                    n_sites)
centerD = centers[good_cluster_name]["centerD"][good_cluster_idx]
centerD_norm2 = np.dot(centerD,centerD)
centerDD = centers[good_cluster_name]["centerDD"][good_cluster_idx]
centerDD_norm2 = np.dot(centerDD,centerDD)
centerD_dot_centerDD = np.dot(centerD,centerDD)
#+END_SRC
Do a first order jitter correction where =h= contains the difference between the event and the center. Obtain the estimated jitter, =jitter0= and the squared norm of the first order corrected residual, =h_order1_norm2= (=<<jitter-order-1>>=): 

#+NAME: classify_and_align_evt-jitter-order-1
#+BEGIN_SRC python :eval no-export
h_order0_norm2 = sum(h**2)
h_dot_centerD = np.dot(h,centerD)
jitter0 = h_dot_centerD/centerD_norm2
h_order1_norm2 = sum((h-jitter0*centerD)**2)     
#+END_SRC
Do a second order jitter correction. Obtain the estimated jitter, =jitter1= and the squared norm of the second order corrected residual, =h_order2_norm2= (=<<jitter-order-2>>=):  

#+NAME: classify_and_align_evt-jitter-order-2
#+BEGIN_SRC python :eval no-export
h_dot_centerDD = np.dot(h,centerDD)
first = -2*h_dot_centerD + \
  2*jitter0*(centerD_norm2 - h_dot_centerDD) + \
  3*jitter0**2*centerD_dot_centerDD + \
  jitter0**3*centerDD_norm2
second = 2*(centerD_norm2 - h_dot_centerDD) + \
  6*jitter0*centerD_dot_centerDD + \
  3*jitter0**2*centerDD_norm2
jitter1 = jitter0 - first/second
h_order2_norm2 = sum((h-jitter1*centerD-jitter1**2/2*centerDD)**2)
#+END_SRC
Now define the function:

#+NAME: classify_and_align_evt
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def classify_and_align_evt(evt_pos, data, centers,
                           before=14, after=30):
    <<classify_and_align_evt-docstring>>
    <<classify_and_align_evt-centersM>>
    <<classify_and_align_evt-cluster_idx>>
    <<classify_and_align_evt-get-centers>>
    h = delta[cluster_idx,:]
    <<classify_and_align_evt-jitter-order-1>>
    if h_order0_norm2 > h_order1_norm2:
        <<classify_and_align_evt-jitter-order-2>>
        if h_order1_norm2 <= h_order2_norm2:
            jitter1 = jitter0
    else:
        jitter1 = 0
    if abs(round(jitter1)) > 0:
        evt_pos -= int(round(jitter1))
        evt = cut_sgl_evt(evt_pos,data=data,
                          before=before, after=after)
        h = evt - centers[good_cluster_name]["center"]\
          [good_cluster_idx]
        <<classify_and_align_evt-jitter-order-1>>  
        if h_order0_norm2 > h_order1_norm2:
            <<classify_and_align_evt-jitter-order-2>>
            if h_order1_norm2 <= h_order2_norm2:
                jitter1 = jitter0
        else:
            jitter1 = 0
    if sum(evt**2) > sum((h-jitter1*centerD-jitter1**2/2*centerDD)**2):
        return [cluster_names[cluster_idx], evt_pos, jitter1]
    else:
        return ['?',evt_pos, jitter1]

#+END_SRC

** =predict_data=
We define function =predict_data= that creates an ideal data trace given events' positions, events' origins and a clusters' catalog. We start with the =docstring=:

#+NAME: predict_data-docstring
#+BEGIN_SRC python :eval no-export
"""Predicts ideal data given a list of centers' names, positions,
jitters and a dictionary of centers.

Parameters
----------
class_pos_jitter_list : a list of lists returned by
                        classify_and_align_evt.
centers_dictionary : a centers' dictionary returned by
                     mk_center_dictionary.
nb_channels : the number of recording channels.
data_length : the number of sampling points.

Returns
-------
A matrix of ideal (noise free) data with nb_channels rows and
data_length columns.
"""
#+END_SRC
And the function:

#+NAME: predict_data
#+BEGIN_SRC python :eval no-export :results silent :noweb no-export
def predict_data(class_pos_jitter_list,
                 centers_dictionary,
                 nb_channels=4,
                 data_length=300000):
    <<predict_data-docstring>>
    ## Create next a matrix that will contain the results
    res = np.zeros((nb_channels,data_length))
    ## Go through every list element
    for class_pos_jitter in class_pos_jitter_list:
        cluster_name = class_pos_jitter[0]
        if cluster_name != '?':
            center = centers_dictionary[cluster_name]["center"]
            centerD = centers_dictionary[cluster_name]["centerD"]
            centerDD = centers_dictionary[cluster_name]["centerDD"]
            jitter = class_pos_jitter[2]
            pred = center + jitter*centerD + jitter**2/2*centerDD
            pred = pred.reshape((nb_channels,len(center)//nb_channels))
            idx = centers_dictionary[cluster_name]["center_idx"] + \
              class_pos_jitter[1]
            ## Make sure that the event is not too close to the
            ## boundaries
            within = np.bitwise_and(0 <= idx, idx < data_length)
            kw = idx[within]
            res[:,kw] += pred[:,within]
    return res

#+END_SRC
