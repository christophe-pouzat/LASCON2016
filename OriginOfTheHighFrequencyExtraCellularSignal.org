# -*- org-export-babel-evaluate: nil; ispell-local-dictionary: "american" -*-
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline
#+OPTIONS: author:t c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+TITLE: Origin Of The (High Frequency) Extra-cellular Signal
#+DATE: <2016-01-14 jeu.>
#+AUTHOR: Christophe Pouzat
#+EMAIL: christophe.pouzat@parisdescartes.fr
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.3)
#+PROPERTY: header-args:python *Python* :session  :results pp

#+NAME: emacs-set-up
#+BEGIN_SRC emacs-lisp :results silent :exports none
(setq py-shell-name "~/anaconda3/bin/ipython")

(defun update-tag ()
  (interactive)
  (save-excursion
    (goto-char (point-min))
    (let ((count 1))
      (while (re-search-forward "\\tag{\\([0-9]+\\)}" nil t)
        (replace-match (format "%d" count) nil nil nil 1)
        (setq count (1+ count)))))
  )
#+END_SRC


* Introduction

We want to explore here the basic properties of the /extra-cellular/ potential generated by a uniform active---that is, able to propagate an action potential (or /spike/)---cable or axon. We are going to use the conductance model of [[http://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1952.sp004764/abstract][Hodgkin and Huxley (1952)]] together with the cable model making the "full" H & H model.

Remember that H & H /did not/ solve their full model in their /opus magnum/, remember also that the /mechanical calculator/ they had at this time was far less powerful that any of the smartphones everyone has nowadays in his/her pocket. They used a "trick" looking at the propagation of a waveform without deformation at a constant speed $\theta$, that is, a spike or action potential. In this way the spatial derivatives of the membrane potential can be expressed as time derivatives (as we will see bellow) and the partial differential equation (PDE) of the full model can be replaced by ordinary differential equations (ODE).

So we are going to start by deriving the expression of the extra-cellular potential generated by a "cable like neurite"---a neurite with a large length to radius ratio---that can approximated by a /line source/. We will follow a classical development that is very clearly explained in the book of Plonsey and Barr (2007) /Bioelectricity. A Quantitative Approach/, published by Springer. This development will lead us to an equation relating the extra-cellular potential to the integral of the weighted second partial derivative of the membrane potential with respect to space, $\partial^2 V_m(x,t) / \partial x^2$. The H & H model will give us actual values for this derivative but that will require a numerical solution. We will then explore the effect of the axonal diameter on the extra-cellular potential. 

* Relation between membrane potential and extracellular potential

** Basic equations

The electrostatic potential $\Phi_e$ [mV] generated by a constant *point source* of intensity $I_0$ [mA] is given by: 
\begin{align}\label{eq:stat}\tag{1} \Phi_e = \frac{1}{4 \pi \sigma_e} \frac{I_0}{r} \, ,\end{align} 
where $\sigma_e$ [S/cm] is the conductivity of the extracellular medium assumed homogeneous and $r$ [cm] is the distance between the source and the electrode (Plonsey and Barr, 2007, /Bioelectricity: A Quantitative Approach/, p. 29).

For an extended source with a large length to diameter ratio (a cable) that can be approximated by a *line source*; the generalization of the previous equation for a continuous line source along the x axis (between $x_{min}$ and $x_{max}$) of the 3D Euclidean space equipped with Cartesian coordinates when the electrode is located at $(X,Y,Z)$ is: 
\begin{align}\label{eq:stat1}\tag{2} \Phi_e(X,Y,Z) = \frac{1}{4 \pi \sigma_e} \int_{x_{min}}^{x_{max}} \frac{i_m(x)}{r(x)} dx \, ,\end{align} 
where: 
\begin{align}\tag{3} r(x) \doteq \sqrt{(x-X)^2+Y^2+Z^2}\;,\end{align} 
and $i_m(x)$ [mA/cm] is the /current density/ at position $x$ [cm] along the cable.

** Membrane current density

We get an expression for $i_m(x)$ by considering a small piece of cable of radius $a$ [cm] and of length $\Delta x$ [cm] (Plonsey and Barr, 2007).

If the intracellular potential at position $x$ is written $\Phi_i(x)$, then Ohm's law---the current equals the potential drop multiplied by the conductance---implies that the /axial current/ $I_i(x)$ [mA] is given by ($\sigma_i$ [S/cm] is the intracellular conductivity):
\begin{align}
    I_i(x) &= -\pi a^2 \sigma_i \frac{\Phi_i(x+\Delta x) -
\Phi_i(x)}{\Delta x} \nonumber \\
            &\xrightarrow[\Delta x \to 0]{ }  -\pi a^2 \sigma_i \frac{d \Phi_i(x)}{dx} \, . \label{eq:stat2}\tag{4}
\end{align}

Then the charge conservation implies that the membrane current density $i_m(x)$ (positive for
an outgoing current) is given by:
\begin{align}
    I_i(x+\Delta x) - I_i(x) &= -i_m(x)\, \Delta{}x \nonumber \\
    \frac{d I_i(x)}{dx} &= -i_m(x). \label{eq:stat3}\tag{5}
\end{align}

Combining equation 4 and equation 5 we get: 
\begin{align}
    \label{eq:stat4}\tag{6}
    i_m(x) &= \pi a^2 \sigma_i \frac{d^2 \Phi_i(x)}{d x^2}\, .
\end{align}

Now, writing the membrane potential $V_m = \Phi_i - \Phi_e$ we have: 
\begin{align}
    \label{eq:stat5}\tag{7}
    i_m(x) &=  \pi a^2 \sigma_i \frac{d^2 V_m(x)}{dx^2} \,.
\end{align}

This allows us to rewrite equation 2 as:
\begin{align}
    \label{eq:stat6}\tag{8}
    \Phi_e(X,Y,Z) =  \frac{a^2 \sigma_i}{4 \sigma_e} \int_{x_{min}}^{x_{max}} \frac{1}{\sqrt{(x-X)^2+Y^2+Z^2}}
    \frac{d^2 V_m(x)}{dx^2} dx \,.    
\end{align}

The quasi-static approximation (Plonsey, 1967, /The bulletin of mathematical biophysics/ *29*:657-664; Nicholson and Freeman, 1975, /Journal of Neurophysiology/ *38*: 356-368)---that 
amounts to considering the extracellular medium as purely resistive--- leads to 
a more general, *time dependent*, version of equation 8:
\begin{align}
    \label{eq:stat7}\tag{9}
    \Phi_e(X,Y,Z,t) =  \frac{a^2 \sigma_i}{4 \sigma_e} \int_{x_{min}}^{x_{max}} \frac{1}{\sqrt{(x-X)^2+Y^2+Z^2}}
    \frac{\partial^2 V_m(x,t)}{\partial x^2} dx \,.    
\end{align}

*Notice that the derivation of equations 8 and 9  does not assume anything about the origin of the membrane potential 
non-uniformity.*

If the membrane potential deviation with respect to rest, $\Delta{}V_m$, and its derivatives are null at the boundaries of the integration domain, then two rounds of integration by part give (with $X=0$ and $h = \sqrt{Y^2+Z^2}$):
\begin{align}
    \label{eq:statPart}\tag{10}
    \Phi_e(h) =  \frac{a^2 \sigma_i}{4 \sigma_e} \int_{x_{min}}^{x_{max}} \left(\frac{3 u^2}{(u^2+h^2)^{5/2}} - \frac{1}{(u^2+h^2)^{3/2}}\right) \Delta{}V_m(u) du \, .
\end{align}

At that stage, in order to go further, we need an explicit expression or value for $\Delta{}V_m$. We are going to solve numerically the H & H equations for that.

* Numerical integration of the H & H equation

We follow here the exposition of Tuckwell (1988) /Introduction to theoretical neurobiology. Volume 2./ CUP, pp 54-70[fn:Ames1977]. We want to solve the following set of equations:

\begin{align}
    C_m \, \frac{\partial V_m}{\partial t} &= \frac{a \sigma_i}{2} \frac{\partial^2 V_m}{\partial x^2} + \overline{g}_K n^4 (V_K-V_m) + \overline{g}_{Na} m^3 h (V_{Na}-V_m) + g_l (V_l - V_m) + I_A \, , \label{eq:HH-PDE}\tag{11}\\
    \frac{\partial n}{\partial t} &= \alpha_n(V_m) (1-n) - \beta_n(V_m) n \, , \label{eq:HH-n}\tag{12}\\
    \frac{\partial m}{\partial t} &= \alpha_m(V_m) (1-m) - \beta_m(V_m) m \, , \label{eq:HH-m}\tag{13}\\
    \frac{\partial h}{\partial t} &= \alpha_h(V_m) (1-h) - \beta_h(V_m) h \, , \label{eq:HH-h}\tag{14}
\end{align}

where $C_m$ is the membrane capacitance per unit area [F/cm$^2$]; $\overline{g}_K$, $\overline{g}_{Na}$ and $g_l$ are the potassium, sodium and leak conductances per unit area [S/cm$^2$]; $V_K$, $V_{Na}$ and $V_l$ are the potassium, sodium and leak currents reversal potentials [mV] and $I_a$ is "externally" applied current per unit area [mA/cm$^2$] and all the $\alpha$ and $\beta$ functions are measured in [1/ms]. 

** A standardized form for the non-linear reaction-diffusion equations 

We will consider a /reaction-diffusion/ system with the form:

\begin{align}
    \mathbf{u}_t = \mathbf{D} \, \mathbf{u}_{xx} + \mathbf{F}(\mathbf{u}) \, , \label{eq:reaction-diffusion}\tag{15}
\end{align}

where the $t$ subscript stands for the partial derivative with respect to time, the $xx$ subscripts stands for the second partial derivative with respect to position, $\mathbf{u} = \left(u_1(x,t),\ldots,u_n(x,t)\right)^T \in \mathbb{R}^n$, $\mathbf{D}$ is a diagonal $n \times n$  matrix of diffusion coefficients $\left(D_1,\ldots,D_n\right)$ and $\mathbf{F}(\cdot) = \left(F_1(\cdot),\ldots,F_n(\cdot)\right)^T$ is a vector-valued function. The corresponds with the above H & H equations is obtained by setting: $\mathbf{u} = \left(V_m,n,m,h\right)^T$; $\left(D_1,D_2,D_3,D_4\right) = \left(\frac{a \sigma_i}{2 C_m},0,0,0\right)$, $F_1(\mathbf{u}) = \left(\overline{g}_K n^4 (V_K-V_m) + \overline{g}_{Na} m^3 h (V_{Na}-V_m) + g_l (V_l - V_m) + I_A\right)/C_m$, $F_2(\mathbf{u}) \equiv F_2(V_m,n)$, $F_3(\mathbf{u}) \equiv F_3(V_m,m)$ and $F_4(\mathbf{u}) \equiv F_4(V_m,h)$ are given by equations 12, 13 and 14.   

*** The heat equation

Let us consider a simpler problem, the /heat equation/:

\begin{align}
    u_t = D \, u_{xx} \, , \label{eq:heat-equation}\tag{16}
\end{align}

where $u(x,t)$ is a scalar. A numerical integration procedure is possible by /finite differencing/. Here, the heat equation (16) is replaced by a finite difference equation whose solution /approximates/ the one of the heat equation. We discretize the $x$ axis using $m+1$ equally spaced points (with a step $\Delta{}x$) and the $t$ axis using $n+1$ equally spaced times (with a step $\Delta{}t$). We write the approximate solution as:

\begin{align}
    U_{i,j} = u(i \Delta{}x,j \Delta{}t) \quad i = 0,\ldots,m \; i = 0,\ldots,n \, . \label{eq:discrete-u}\tag{17}
\end{align}

The finite difference approximations of the required derivatives are:

\begin{align}
    u_t(x,t) &\approx \frac{U_{i,j+1}-U_{i,j}}{\Delta{}t} \, , \label{eq:u_t}\tag{18} \\
    u_x(x,t) &\approx \frac{U_{i+1,j}-U_{i,j}}{\Delta{}x} \, , \label{eq:u_x}\tag{19} \\
    u_{xx}(x,t) &\approx \frac{u_x(x,t)-u_x(x-\Delta{}x,t)}{\Delta{}x} \, , \nonumber \\
    &\approx \frac{U_{i+1,j}-2 \, U_{i,j} + U_{i-1,j}}{\Delta{}x^2} \, . \label{eq:u_xx}\tag{20} \\
\end{align}
 
The numerical integration of the heat equation with the finite difference equation is obtained by establishing a relation between the $U_{i,j+1}$ and the $U_{i,j}$. One methods approximates the second spatial derivative at $t$ by the one at $t+\Delta{}t$ giving the scheme:

\begin{align}
    \frac{U_{i,j+1}-U_{i,j}}{\Delta{}t} = \frac{D}{\Delta{}x^2} \left(U_{i+1,j+1}-2 \, U_{i,j+1} + U_{i-1,j+1}\right)\, . \label{eq:Ames-scheme}\tag{21} 
\end{align}
 
[[https://en.wikipedia.org/wiki/Crank–Nicolson_method][Crank and Nicolson]] used the average of the approximations to the second space derivatives at the $jth$ and $(j+1)th$ time points to get:

\begin{align}
    \frac{U_{i,j+1}-U_{i,j}}{\Delta{}t} = \frac{D}{2 \Delta{}x^2} \left(U_{i+1,j+1}-2 \, U_{i,j+1} + U_{i-1,j+1} + U_{i+1,j}-2 \, U_{i,j} + U_{i-1,j}\right)\, . \label{eq:Crank-Nicolson}\tag{22} 
\end{align}

More generally a weight factor $\lambda$ can be used with weight $\lambda$ for the $(j+1)th$ time points and weight $(1-\lambda)$ for the $jth$ with $0 \le \lambda \le 1$. Then with:

\begin{align}
    r \doteq \frac{D \Delta{}t}{\Delta{}x^2} \, , \label{eq:step-ratio}\tag{23} 
\end{align}

we have:

\begin{align}
    -r \lambda U_{i-1,j+1} + (1+2 r \lambda) U_{i,j+1} -r \lambda U_{i+1,j+1} = r (1-\lambda) U_{i-1,j} + \left(1-2 r (1-\lambda)\right) U_{i,j} + r (1-\lambda) U_{i+1,j}\, , \label{eq:general-Crank-Nicolson}\tag{23} 
\end{align}

where all the unknown terms in $j+1$ are on the left side. Since $i = 0,1,\ldots,m$ there are $m+1$ equations with $m+1$ unknown. This integration scheme is called /implicit/ because a linear system must be solved to obtain the values of $u(x,t)$ at the next time step. The system defined by equation 23 is /tridiagonal/ and can be solved without matrix inversion. In =Python=, the [[http://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html][scipy.linalg]] sub-module provides the [[http://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_banded.html#scipy.linalg.solve_banded][solve_banded]] function to work efficiently with linear systems exhibiting a banded structure. 

*** Adding the reaction term: Lee's method

We now add a /reaction term/ $F(u)$ to the scalar heat equation:

\begin{align}
    u_t = D \, u_{xx} + F(u) \, . \label{eq:heat-equation-plus-reaction}\tag{24}
\end{align}

In the Crank-Nicolson method the second space derivative is approximated by the average of its finite-difference approximations at time points $j$ and $j+1$. A similar estimate is needed for $F(u)$; in other words we need $F(U_{i,j+^1/_2})$ and we approximate $U_{i,j+^1/_2}$ by:

\begin{align}
    U_{i,j+^1/_2} &\approx U_{i,j} + (U_{i,j} - U_{i,j-1})/2  \nonumber \\
    &\approx \frac{3}{2} U_{i,j} - \frac{1}{2} U_{i,j-1} \, . \label{eq:mid-point}\tag{25}
\end{align}

And Lees' modification of the Crank-Nicolson method gives the tridiagonal system (remember that $\lambda$ in equation 23 equals $^1/_2$ for the Crank-Nicolson method):

\begin{align}
    -\frac{r}{2} U_{i-1,j+1} + (1+r) U_{i,j+1} -\frac{r}{2} U_{i+1,j+1} = \frac{r}{2} U_{i-1,j} + (1-r) U_{i,j} + \frac{r}{2}U_{i+1,j} + \Delta{}t F\left(\frac{3}{2} U_{i,j} - \frac{1}{2} U_{i,j-1}\right)\, . \label{eq:Lees-method}\tag{26} 
\end{align}

Clearly this last equation can only be used if $j>0$ so for $j=0$ we use an /explicit/ version (we use $j$ instead of $j+1$ in the right hand side of equation 21):

\begin{align}
    U_{i,1} = r \left(U_{i-1,0} -2 U_{i,0} + U_{i+1,0}\right) + \Delta{}t F\left(U_{i,0}\right) + U_{i,0}\, . \label{eq:Lees-method-explicit}\tag{27} 
\end{align}

*** Boundary conditions

There is still one problem to consider before starting writing our code: the /boundary conditions/, that is what happens at the ends of the cable. There two "extreme" possibilities (and a third one in between the two). The first possibility consists in imposing the voltage at both ends, this leads to the /Dirichlet conditions/:

\begin{align}
    u(0,t) &= \alpha \, ,  \label{eq:Dirichlet-0}\tag{28} \\
    u(L,t) &= \beta   \, . \label{eq:Dirichlet-L}\tag{29}
\end{align}

The finite difference version is:

\begin{align}
    U_{0,j} &= \alpha \, , \quad j=0,1,\ldots  \, , \label{eq:Dirichlet-0-discrete}\tag{30} \\
    U_{m,j} &= \beta \, , \quad j=0,1,\ldots   \, . \label{eq:Dirichlet-L-discrete}\tag{31}
\end{align}

These conditions reduce the number of unknown in our linear system by 2, from $m+1$ to $m-1$ and correspond to voltage-clamping the ends of the cable.

The more common conditions in simulation studies are the /Neumann conditions/ where the values of the space derivatives of the potential are imposed at the ends:

\begin{align}
    u_x(0,t) &= \alpha \, ,  \label{eq:Neumann-0}\tag{32} \\
    u_x(L,t) &= \beta   \, . \label{eq:Neumann-L}\tag{33}
\end{align}

The common values chosen are $\alpha = \beta = 0$ often referred to as the "sealed ends" conditions---the ones we are going to choose in our numerical implementation. To get the finite difference version, a quick solution would be using $u_x(x,t) \approx \left(U_{i+1,j}-U_{i,j}\right) / \Delta{}x$, but we can do better---in term of the approximation of the space derivative by its finite difference version at fixed $\Delta{}x$ using:

\begin{align}
    u_x(i \Delta{}x,j \Delta{}t) &\approx \frac{U_{i+1,j} - U_{i-1,j}}{2 \Delta{}x}   \, . \label{eq:central-difference}\tag{34}
\end{align}

Can you see why? Then the Neumann conditions become:

\begin{align}
    U_{-1,j} &= -2 \alpha \Delta{}x + U_{1,j}\, ,  \label{eq:Neumann-0-discrete}\tag{35} \\
    U_{m+1,j} &= 2 \beta \Delta{}x + U_{m-1,j}  \, . \label{eq:Neumann-L-discrete}\tag{36}
\end{align}

This amounts to introducing "false boundaries" and substituting 35 in 26, the first equation becomes (for $j>0$):

\begin{align}
     (1+r) U_{0,j+1} -r U_{1,j+1} = - 2 r \alpha \Delta{}x + (1-r) U_{0,j} + r U_{1,j} + \Delta{}t F\left(\frac{3}{2} U_{0,j} - \frac{1}{2} U_{0,j-1}\right)\, . \label{eq:Lees-left}\tag{37} 
\end{align}
 
At $j=0$ the substitution in equation 27 leads to:

\begin{align}
     U_{0,1} = 2 r \left(U_{1,0} - U_{0,0} - \alpha \Delta{}x\right) + \Delta{}t F\left(U_{0,0}\right) + U_{0,0}\, . \label{eq:Lees-left-at-0}\tag{38} 
\end{align}

At the other end we get for $j>0$:

\begin{align}
     -r U_{m-1,j+1} + (1+r) U_{m,j+1} = 2 r \beta \Delta{}x + r U_{m-1,j} + (1-r) U_{m,j} + \Delta{}t F\left(\frac{3}{2} U_{m,j} - \frac{1}{2} U_{m,j-1}\right)\, , \label{eq:Lees-right}\tag{39} 
\end{align}

while for $j=0$ we have:

\begin{align}
     U_{m,1} = 2 r \left(U_{m-1,0} - U_{m,0} + \beta \Delta{}x\right) + \Delta{}t F\left(U_{m,0}\right) + U_{m,0}\, . \label{eq:Lees-right-at-0}\tag{40} 
\end{align}

 
*** Python code doing the job
We are going to solve the standard H & H model using the Neumann boundary conditions with $\alpha = \beta = 0$ ("sealed ends"). We start by an =IPython= session---but it wokrs as well with a classical =Python= session---loading the two main modules we are going on a regular basis, =numpy= and =pylab= a sub-module of =matplotlib=:

#+NAME+: start-session
#+BEGIN_SRC python :eval no-export :results silent
import numpy as np
import matplotlib.pylab as plt
plt.ion()
plt.style.use('ggplot')
#+END_SRC

The two last commands give us /interactive/ graphics (=plt.ion=) and a nicer default style for the graphs (=plt.style.use('ggplot')=). We then assign a few variables considering an axon with a radius $a$ of 1 $\mu{}m$ that is $10^{-4}$ cm (for quantitative data on CNS axons diameters, see [[http://www.jneurosci.org/content/32/2/626.abstract][Perge et al (2013)]]):

#+NAME: assign-a-axon-radius
#+BEGIN_SRC python :eval no-export :results silent
a = 1e-4
#+END_SRC

#+NAME: Cm-rho-D-definition
#+BEGIN_SRC python :eval no-export :exports both
Cm = 1.0 # H & H 1952 [μF / cm^2]
rho = 35.4 # H & H 1952, rho is the inverse of σi [Ω cm]
D = a / (2.0 * rho * Cm) # the "Diffusion" constant
D
#+END_SRC 

#+RESULTS: Cm-rho-D-definition
: 1.4124293785310736e-06

Notice that with this choice of units =D= is measured in cm$^2$ / $\mu{}s$. We define next, for each activation variable, $n, m, h$ the $\alpha(v)$ and $\beta(v)$ functions---where the formal parameter $v$ stands for the *deviation of the membrane voltage with respect to rest*---as well as a function returning the steady-state value of the variable at a given voltage. We start with the $n$ activation variable---the =numpy= module must have been previously imported with the alias =np= (=import numpy as np=)---:

#+NAME: n-activation
#+BEGIN_SRC python :eval no-export :results silent
def alpha_n(v):
    if np.abs(v-10.0) < 1e-10:
        return 0.1
    else:
        return 0.01*(10.0 - v)/(np.exp((10.0-v)/10.0)-1.0)
def beta_n(v):
    return 0.125*np.exp(-0.0125*v)
n_inf = np.vectorize(lambda v: alpha_n(v)/(alpha_n(v) + beta_n(v)))
#+END_SRC

Notice that we took care of the special case $v=10$ using the limit to avoid the undefined expression $0/0$. The =n_inf= function has been defined in a [[http://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html#numpy.vectorize][vectorized form]] since our definition of =alpha_n= works only with scalar arguments. Having defined these functions it is always a good idea to make a couple of graphs to make sure that we did things properly (we should get figures 4 and 5, p 511 of H & H 1952; /don't forget that the membrane voltage convention at that time was the opposite of the one now used/):

#+NAME: graph-n-activation
#+BEGIN_SRC python :eval no-export :results silent
vv = np.linspace(-50,110,201)
plt.plot(vv,np.vectorize(alpha_n)(vv),lw=2)
plt.plot(vv,np.vectorize(beta_n)(vv),lw=2)
plt.plot(vv,n_inf(vv),lw=2)
#+END_SRC

#+NAME: save-graph-n-activation
#+BEGIN_SRC python :eval no-exports :results file :exports results
plt.savefig('figsL1/n_activation.png')
plt.close()
'figsL1/n_activation.png'
#+END_SRC

#+CAPTION: $\alpha_n$ (red), $\beta_n$ (blue) and $n_{\infty}$ (violet) as a function of the membrane voltage deviation with respect to rest. 
#+RESULTS: save-graph-n-activation
[[file:figsL1/n_activation.png]]

We can now define function =F_n= corresponding to the $F_2$ of equation 15 whose expression is given by equation 12; this function takes two formal parameters: the membrane potential (deviation) =v= and the activation variable =n=:

#+NAME: F_n-and-vF_n-definitions 
#+BEGIN_SRC python :eval no-export :results silent
def F_n(v,n):
    if np.abs(v-10.0) < 1e-10:
        alpha = 0.1
    else:
        alpha = 0.01*(10.0 - v)/(np.exp((10.0-v)/10.0)-1.0)
    beta = 0.125*np.exp(-0.0125*v)
    return alpha*(1-n)-beta*n
vF_n = np.vectorize(F_n)    
#+END_SRC
 
It is again a good idea to use these newly defined functions to make sure that nothing "too pathological" happens:

#+NAME: call-F_n
#+BEGIN_SRC python :eval no-export :exports both
F_n(20,0.6)
#+END_SRC

#+RESULTS: call-F_n
: 0.0048690095444177128

#+NAME: call-vF_n
#+BEGIN_SRC python :eval no-export :exports both
vF_n([-10,0,10,20,30],[0.1,0.2,0.3,0.4,0.5])
#+END_SRC

#+RESULTS: call-vF_n
: array([ 0.01400882,  0.02155814,  0.03690637,  0.05597856,  0.07269618])

Notice that we "redefine" =alpha_n= and =beta_n= inside =F_n=, this is to gain execution time by avoiding function calls. We also define a vectorized version =vF_n= that will take two formal parameters, =v= and =n=, that can be vectors. We proceed in the same way with the $m$ activation variable:

#+NAME: m-activation
#+BEGIN_SRC python :eval no-export :results silent
def alpha_m(v):
    if np.abs(v-25.0) < 1e-10:
        return 1.0
    else:
        return 0.1*(25.0 - v)/(np.exp((25.0 - v)/10.0)-1.0)
def beta_m(v):
        return 4*np.exp(-.0555*v)
m_inf = np.vectorize(lambda v: alpha_m(v)/(alpha_m(v) + beta_m(v)))
#+END_SRC

The graphs (not shown) giving figures 7 and 8 pp 515-516 are obtained with:

#+NAME: graph-m-activation
#+BEGIN_SRC python :eval no-export :results silent
vv = np.linspace(-50,110,201)
plt.plot(vv,np.vectorize(alpha_m)(vv),lw=2)
plt.plot(vv,np.vectorize(beta_m)(vv),lw=2)
plt.plot(vv,m_inf(vv)*10,lw=2)
plt.xlim(-10,110)
plt.ylim(0,10)
#+END_SRC

#+NAME: F_m-and-vF_m-definitions 
#+BEGIN_SRC python :eval no-export :results silent
def F_m(v,m):
    if np.abs(v-25.0) < 1e-10:
        alpha = 1.0
    else:
        alpha =  0.1*(25.0 - v)/(np.exp((25.0 - v)/10.0)-1.0)
    beta = 4*np.exp(-.0555*v)
    return alpha*(1-m)-beta*m
vF_m = np.vectorize(F_m)
#+END_SRC

A quick check gives:

#+NAME: call-vF_m
#+BEGIN_SRC python :eval no-export :exports both
vF_m([-10,0,10,20,30],[0.1,0.2,0.3,0.4,0.5])
#+END_SRC

#+RESULTS: call-vF_m
: array([-0.59869277, -0.62114902, -0.38730895, -0.06484611,  0.2569922 ])

And for the $h$ activation variable:

#+NAME: h-activation
#+BEGIN_SRC python :eval no-export :results silent
def alpha_h(v):
    return 0.07*np.exp(-0.05*v)
def beta_h(v):
    return 1.0/(np.exp((30.0 - v)/10.0) + 1.0)
def h_inf(v):
    return alpha_h(v)/(alpha_h(v) + beta_h(v))
#+END_SRC

Notice that since =alpha_h= is already (implicitly) vectorized, there is no need to use =np.vectorize= when defining function =h_inf=. The graphs (not shown) giving figures 9 and 10 pp 517-518 are obtained with:

#+NAME: graph-h-activation
#+BEGIN_SRC python :eval no-export :results silent
vv = np.linspace(-50,110,201)
plt.plot(vv,np.vectorize(alpha_h)(vv),lw=2)
plt.plot(vv,np.vectorize(beta_h)(vv),lw=2)
plt.plot(vv,h_inf(vv),lw=2)
#+END_SRC

#+NAME: F_h-and-vF_h-definitions 
#+BEGIN_SRC python :eval no-export :results silent
def F_h(v,h):
    return 0.07*np.exp(-0.05*v)*(1-h)-1.0/(np.exp((30.0 - v)/10.0) + 1.0)*h
vF_h = np.vectorize(F_h)
#+END_SRC

A quick check gives:

#+NAME: call-vF_h
#+BEGIN_SRC python :eval no-export :exports both
vF_h([-10,0,10,20,30],[0.1,0.2,0.3,0.4,0.5])
#+END_SRC

#+RESULTS: call-vF_h
: array([ 0.10207082,  0.04651483, -0.00604087, -0.09212563, -0.24219044])

We define next =F_V= corresponding to the $F_1$ of equation 15. This function takes 5 formal parameters: =v=, =n=, =m=, =h= and =Ia= the injected current. The maximal conductances [mS / cm$^2$] and reversal potentials [mV] from H & H (1952) are assigned to local variables in the function. A vectorized version is also defined:

#+NAME: F_V-and-vF_V-definitions
#+BEGIN_SRC python :eval no-export :results silent
def F_V(v,n,m,h,Ia):
    GNa, GK, GL = 120.0, 36.0, 0.3 # H & H 1952
    ENa, EK, EL = 115.0, -12.0, 10.5987 # H & H 1952
    return (GK*n**4*(EK-v)+GNa*m**3*h*(ENa-v)+GL*(EL-v)+Ia)/Cm
vF_V = np.vectorize(F_V)
#+END_SRC

We can now make a first (explicit) step. We are going to consider a thin cable with a 1 $\mu{}m$ radius and we start by getting its length constant: $\lambda = \sqrt{a/2 \rho_i \sigma_m}$. We already set $\rho_i = 35.4$ [$\Omega{}$ cm], we get the resting value of $\sigma_m$ [S / cm$^2$] by getting the activation variables values at resting level (don't forget that the conductance densities given by H & H are in [mS]):

#+NAME: sigma_m_rest
#+BEGIN_SRC python :eval no-export :exports both
sigma_m_rest = (36*n_inf(0)**4+120*m_inf(0)**3*h_inf(0)+0.3)/1000
sigma_m_rest
#+END_SRC

#+RESULTS: sigma_m_rest
: 0.00067725364844574128

This gives us a length constant at rest in cm:

#+NAME: lambda_rest
#+BEGIN_SRC python :eval no-export :exports both
lambda_rest = np.sqrt(1e-4/2/rho/sigma_m_rest)
lambda_rest
#+END_SRC

#+RESULTS: lambda_rest
: 0.045667548060889344

So our length constant is roughly 500 $\mu{}m$. We will pick a space discretization step of 50 $\mu{}m$ (5 $\times 10^{-3}$ cm) equal to a tenth of the length constant and choose a cable length of 20000 $\mu{}m$ (2 cm), forty times the length constant. We then choose our time discretization step such that the value $r$ defined by equation 23 is not too large, say 4 (the reason for using an implicit method like the [[https://en.wikipedia.org/wiki/Crank–Nicolson_method][Crank-Nicolson]] method instead of an explicit one in that the latter is stable only if $r \le 0.5$). That gives us for $\Delta{}t$ (remember that our =D= above is in cm$^2$ / $\mu{}s$ and we want a result in $ms$):

#+NAME: Delta_t-and-Delta_x
#+BEGIN_SRC python :eval no-export :exports both 
Delta_x = 5e-3
r = 2
Delta_t = r*Delta_x**2/D/1000
Delta_t
#+END_SRC 

#+RESULTS: Delta_t-and-Delta_x
: 0.0354

To be on the safe side, we will pick a $\Delta{}t$ of 0.025 ms:

#+NAME: define-Delta_t
#+BEGIN_SRC python :eval no-export :results silent
Delta_t = 0.025
#+END_SRC
 
We now need 4 vectors containing the membrane voltage (deviation) and the value of each activation variable at each discrete location along our cable:

#+NAME: define-v_0-n_0-m_0-h_0
#+BEGIN_SRC python :eval no-export :results silent
L = 2
M = L/Delta_x
v_0 = np.zeros(M+1)
n_0 = np.ones(M+1)*n_inf(0)
m_0 = np.ones(M+1)*m_inf(0)
h_0 = np.ones(M+1)*h_inf(0)
#+END_SRC

We also need a vector of the same length with the injected current density at each point along the axon:

#+NAME: define-Ia_0
#+BEGIN_SRC python :eval no-export :results silent
Ia_0 = np.zeros(M+1)
Ia_0[0] = 1000.0
#+END_SRC

We can now define a function performing a single time step with the explicit method using equations 27, 38 and 40:

#+NAME: explicit_step-definition
#+BEGIN_SRC python :eval no-export :results silent
def explicit_step(v,n,m,h,Ia):
    v_new = np.copy(v)
    n_new = np.copy(n)
    m_new = np.copy(m)
    h_new = np.copy(h)
    reaction_term = Delta_t * vF_V(v,n,m,h,Ia)
    diffusion_term = np.zeros(len(v))
    diffusion_term[1:-1] = (v[0:-2]-2*v[1:-1]+v[2:])*r
    diffusion_term[0] = 2*r*(v[1]-v[0])
    diffusion_term[-1] = 2*r*(v[-2]-v[-1])
    v_new += diffusion_term + reaction_term
    n_new += Delta_t*vF_n(v,n)
    m_new += Delta_t*vF_m(v,m)
    h_new += Delta_t*vF_h(v,h)
    return v_new,n_new,m_new,h_new
#+END_SRC

We perform one explicit step with:

#+NAME: make-one-explict-step
#+BEGIN_SRC python :eval no-export :results silent
v_1, n_1, m_1, h_1 = explicit_step(v_0,n_0,m_0,h_0,Ia_0)
#+END_SRC

The general time step using Lees' method is an implicit one and requires a banded matrix (containing the voltage factor on the right hand side of equations 28, 37 and 39) to be define that's what do now:

#+NAME: A-definition
#+BEGIN_SRC python :eval no-export :results silent
A = np.zeros((3,M+1))
A[0,2:] = -r/2.0 # upper diagonal
A[0,1] = -r # upper diagonal
A[1,:] = 1.0 + r # diagonal
A[2,:-3] = -r/2.0 # lower diagonal
A[2,-2] = -r # lower diagonal
#+END_SRC

We now define a function doing one Lees' step. The function needs the present and previous (or old) values of v, n, m and h as well as Ia. The function assumes that the banded matrix =A= above is already available in the environment and loads function =solve_banded= from =scipy.linalg= sub-module:

#+NAME: lees_step-definition 
#+BEGIN_SRC python :eval no-export :results silent
def lees_step(v_old,n_old,m_old,h_old,Ia_old,
              v_present,n_present,m_present,h_present,Ia_present):
    from scipy.linalg import solve_banded
    v_extra = 1.5*v_present-0.5*v_old # extrapolated mid-point value
    n_extra = 1.5*n_present-0.5*n_old # extrapolated mid-point value
    m_extra = 1.5*m_present-0.5*m_old # extrapolated mid-point value          
    h_extra = 1.5*h_present-0.5*h_old # extrapolated mid-point value
    Ia_extra = 1.5*Ia_present-0.5*Ia_old # extrapolated mid-point value
    n_new = np.copy(n_present)+Delta_t*vF_n(v_extra,n_extra)
    m_new = np.copy(m_present)+Delta_t*vF_m(v_extra,m_extra)
    h_new = np.copy(h_present)+Delta_t*vF_h(v_extra,h_extra)
    reaction_term = Delta_t*vF_V(v_extra,n_extra,m_extra,h_extra,Ia_extra)
    diffusion_term = (1-r)*np.copy(v_present)
    diffusion_term[1:-1] += (v_present[0:-2] + v_present[2:])*r/2.0
    diffusion_term[0] += r*v_present[1]
    diffusion_term[-1] += r*v_present[-2]
    v_new = solve_banded((1,1),A,reaction_term+diffusion_term)
    return v_new, n_new, m_new, h_new
#+END_SRC

We make one step with:

#+NAME: make-one-lees-step
#+BEGIN_SRC python :eval no-export :results silent
v_2,n_2,m_2,h_2 = lees_step(v_0,n_0,m_0,h_0,Ia_0,v_1,n_1,m_1,h_1,Ia_0)
#+END_SRC

Now 2000 more steps stopping the stimulation after 2 ms or 80 steps (this take a few seconds on my slow laptop):

#+NAME: make-2000-lees-step
#+BEGIN_SRC python :eval no-export :results silent
v_M = np.zeros((2002,int(M+1)))
v_M[0] = v_0
v_M[1] = v_1
n_M = np.zeros((2002,int(M+1)))
n_M[0] = n_0
n_M[1] = n_1
m_M = np.zeros((2002,int(M+1)))
m_M[0] = m_0
m_M[1] = m_1
h_M = np.zeros((2002,int(M+1)))
h_M[0] = h_0
h_M[1] = h_1
for i in range(2,2002):
    if i < 80:
        v_M[i,:],n_M[i,:],m_M[i,:],h_M[i,:] = lees_step(v_M[i-2,:],n_M[i-2,:],m_M[i-2,:],h_M[i-2,:],Ia_0,
                                                        v_M[i-1,:],n_M[i-1,:],m_M[i-1,:],h_M[i-1,:],Ia_0)
    if i == 80:
        v_M[i,:],n_M[i,:],m_M[i,:],h_M[i,:] = lees_step(v_M[i-2,:],n_M[i-2,:],m_M[i-2,:],h_M[i-2,:],Ia_0,
                                                        v_M[i-1,:],n_M[i-1,:],m_M[i-1,:],h_M[i-1,:],0)
    if i > 80:
        v_M[i,:],n_M[i,:],m_M[i,:],h_M[i,:] = lees_step(v_M[i-2,:],n_M[i-2,:],m_M[i-2,:],h_M[i-2,:],0,
                                                        v_M[i-1,:],n_M[i-1,:],m_M[i-1,:],h_M[i-1,:],0)

#+END_SRC

We can graph the spatial profile of the membrane potential deviation at different times like every 40 time steps or every ms for the first 10 ms:

#+NAME: graph-voltage-spatial-profile-evolution
#+BEGIN_SRC python :eval no-export :results silent
xx = np.arange(0,M+1)*5e-3
for i in range(0,442,40):
    plt.plot(xx,v_M[i],color='black',lw=2)
plt.xlabel('Position (cm)')
plt.ylabel(r'$\Delta{}V_m$ (mV)')
#+END_SRC

#+NAME: save-graph-voltage-spatial-profile-evolution
#+BEGIN_SRC python :eval no-exports :results file :exports results
plt.savefig('figsL1/voltage_spatial_profile_evolution.png')
plt.close()
'figsL1/voltage_spatial_profile_evolution.png'
#+END_SRC

#+CAPTION: Spatial profile of the membrane voltage at every ms for 11 ms (from left to right).  
#+RESULTS: save-graph-voltage-spatial-profile-evolution
[[file:figsL1/voltage_spatial_profile_evolution.png]]

Before going further, writing a couple of functions abstracting the many pieces of code we have just used seems a good idea.

*** Some functions definitions

We want a function that takes axon geometrical parameters---radius and length---, simulation time, space and time steps and applied current as formal parameters and for which all the other parameters (reversal potentials, conductances, etc) are set. If we want to be able to change one or several of these other parameters, it is worth exploiting one of the great features of [[https://en.wikipedia.org/wiki/Python_syntax_and_semantics][Python]]: is supports [[https://en.wikipedia.org/wiki/Closure_(computer_programming)][lexical closures]]; and that allows us to write functions returning other functions. That's what we will do here (remark that all the functions previously defined are reused directly, except =F_V= since the necessary parameters are in the lexical scope of the function definition).

#+NAME: mk_cable_fcts-definition
#+BEGIN_SRC python :eval no-export :results silent :noweb yes
def mk_cable_fcts(Cm = 1.0,
                  rho = 35.4, 
                  GNa = 120.0,
                  ENa = 115.0,
                  GK = 36.0,
                  EK = -12.0,
                  GL = 0.3,
                  EL = 10.5987):
    """Returns functions for H & H axon simulation
    
    Formal parameters:
    Cm: a double, the membrane capacitance [μF / cm^2]
    rho: a double, intracellular resistivity [Ω cm]
    GNa: sodium conductance density [mS / cm^2]
    ENa: sodium reversal potential [mV]
    GK: potassium conductance density [mS / cm^2]
    EK: potassium reversal potential [mV]
    GL: leak conductance density [mS / cm^2]
    EL: leak reversal potential [mV]

    Returns:
    D_fct: a function of the axon radius in cm that
           returns the "diffusion coefficient"
    r_fct: a function of the radius, the space and time steps
           that returns the value of r in equation 23
    lambda_fct: a function of the radius that returns the
                length constant
    sim_with_lees: a function of the radius, the length, the steps
                   the injected current that performs the simulation
    """
    import numpy as np
    def D_fct(a):
        return a / (2.0 * rho * Cm)
    <<n-activation>>
    <<F_n-and-vF_n-definitions>>
    <<m-activation>>
    <<F_m-and-vF_m-definitions>>
    <<h-activation>>
    <<F_h-and-vF_h-definitions>>
    def F_V(v,n,m,h,Ia):
        return (GK*n**4*(EK-v)+GNa*m**3*h*(ENa-v)+GL*(EL-v)+Ia)/Cm
    vF_V = np.vectorize(F_V)
    def lambda_fct(a):
        sigma_m_rest = (GK*n_inf(0)**4+GNa*m_inf(0)**3*h_inf(0)+GL)/1000
        return np.sqrt(a/2/rho/sigma_m_rest)
    def r_fct(a,Delta_x,Delta_t):
        return D_fct(a)*Delta_t*1000/Delta_x**2
    <<explicit_step-definition>>
    <<lees_step-definition>>
    def sim_with_lees(a,L,duration,
                      Delta_x,Delta_t,
                      Ia_amp, Ia_duration):
        r = r_fct(a,Delta_x,Delta_t)
        M = int(np.ceil(L/Delta_x))
        N = int(np.ceil(duration/Delta_t))
        Na = int(np.ceil(Ia_duration/Delta_t))
        v_0 = np.zeros(M+1)
        n_0 = np.ones(M+1)*n_inf(0)
        m_0 = np.ones(M+1)*m_inf(0)
        h_0 = np.ones(M+1)*h_inf(0)
        Ia_0 = np.zeros(M+1)
        Ia_0[0] = Ia_amp
        v_1, n_1, m_1, h_1 = explicit_step(v_0,n_0,m_0,h_0,Ia_0)
        A = np.zeros((3,M+1))
        A[0,2:] = -r/2.0 # upper diagonal
        A[0,1] = -r # upper diagonal
        A[1,:] = 1.0 + r # diagonal
        A[2,:-3] = -r/2.0 # lower diagonal
        A[2,-2] = -r # lower diagonal
        v_M = np.zeros((N,int(M+1)))
        v_M[0] = v_0
        v_M[1] = v_1
        n_M = np.zeros((N,int(M+1)))
        n_M[0] = n_0
        n_M[1] = n_1
        m_M = np.zeros((N,int(M+1)))
        m_M[0] = m_0
        m_M[1] = m_1
        h_M = np.zeros((N,int(M+1)))
        h_M[0] = h_0
        h_M[1] = h_1
        for i in range(2,N):
            if i < Na:
                v_M[i,:],n_M[i,:],m_M[i,:],h_M[i,:] = lees_step(v_M[i-2,:],n_M[i-2,:],m_M[i-2,:],h_M[i-2,:],Ia_0,
                                                                v_M[i-1,:],n_M[i-1,:],m_M[i-1,:],h_M[i-1,:],Ia_0)
            if i == Na:
                v_M[i,:],n_M[i,:],m_M[i,:],h_M[i,:] = lees_step(v_M[i-2,:],n_M[i-2,:],m_M[i-2,:],h_M[i-2,:],Ia_0,
                                                                v_M[i-1,:],n_M[i-1,:],m_M[i-1,:],h_M[i-1,:],0)
            if i > Na:
                v_M[i,:],n_M[i,:],m_M[i,:],h_M[i,:] = lees_step(v_M[i-2,:],n_M[i-2,:],m_M[i-2,:],h_M[i-2,:],0,
                                                                v_M[i-1,:],n_M[i-1,:],m_M[i-1,:],h_M[i-1,:],0)
        return v_M,n_M,m_M,h_M
    return D_fct, r_fct, lambda_fct, sim_with_lees
#+END_SRC 

Once this kind of function has been defined *the first thing to do* is to check that it gives the same results as we got before doing the job step by step:

#+NAME: check-mk_cable_fcts
#+BEGIN_SRC python :eval no-export :results silent
D1,r1,lambda1,sim1 = mk_cable_fcts()
D1(1e-4)
r1(1e-4,5e-3,0.025)
v1,n1,m1,h1 = sim1(1e-4,2,15,5e-3,0.025,1000,2)
for i in range(0,442,40):
    plt.plot(v1[i],color='black',lw=2)
#+END_SRC

The results are not shown since they are identical to the previous ones but you are invited to check for yourself.

[fn:Ames1977] Tuckwell follows very closely---with due citations---the treatment of William F. Ames (1977) /NUMERICAL METHODS FOR PARTIAL DIFFERENTIAL EQUATIONS/ Academic Press.
